{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/xingyan/anaconda3/envs/py37/lib/python3.7/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from tqdm import tqdm\n",
    "from torch.optim import Adam\n",
    "from torchvision.transforms import Compose, ToTensor, Normalize, Lambda\n",
    "import numpy as np\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Net(torch.nn.Module):\n",
    "    def __init__(self, dims, device=None):\n",
    "        super().__init__()\n",
    "        self.layers = []\n",
    "        for d in range(len(dims) - 1): \n",
    "            self.layers += [Layer(dims[d], dims[d + 1], id=d, device=device)]\n",
    "\n",
    "    def predict(self, x):\n",
    "        goodness_per_label = []\n",
    "        for label in range(16):\n",
    "            h = x\n",
    "            goodness = []\n",
    "            for layer in self.layers:\n",
    "                h = layer(h)\n",
    "                goodness += [h.pow(2).mean(1)]\n",
    "            goodness_per_label += [sum(goodness).unsqueeze(1)]\n",
    "        goodness_per_label = torch.cat(goodness_per_label, 1)\n",
    "        return goodness_per_label.argmax(1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        return x\n",
    "\n",
    "    def train(self, x_pos, x_neg):\n",
    "        h_pos, h_neg = x_pos, x_neg\n",
    "        for i, layer in enumerate(self.layers):\n",
    "            print(f\"training layer {i}\")\n",
    "            h_pos, h_neg = layer.train(h_pos, h_neg)\n",
    "\n",
    "class Layer(nn.Linear):\n",
    "    def __init__(self, in_features, out_features, id,\n",
    "                 bias=True, device=None, dtype=None):\n",
    "        super().__init__(in_features, out_features, bias, device, dtype)\n",
    "        self.id = id\n",
    "        self.relu = torch.nn.ReLU()\n",
    "        self.opt = Adam(self.parameters(), lr=0.03)\n",
    "        self.threshold = 2.0\n",
    "        self.num_epochs = 1000\n",
    "        self.device = device\n",
    "\n",
    "    def forward(self, x):\n",
    "        x_direction = torch.nn.functional.normalize(x)\n",
    "        x_direction = torch.tensor(x_direction, dtype=torch.float32)\n",
    "        return self.relu(\n",
    "            torch.matmul(x_direction, self.weight.T) + self.bias.unsqueeze(0))\n",
    "\n",
    "    def train(self, x_pos, x_neg):\n",
    "        if self.id == 2:\n",
    "            for _ in tqdm(range(self.num_epochs)):\n",
    "                g_pos = self.Channel(self.forward(x_pos))\n",
    "                g_neg = self.Channel(self.forward(x_neg))\n",
    "                g_pos_sum = g_pos[:, :128].pow(2) + g_pos[:, 128:].pow(2)\n",
    "                g_neg_sum = g_neg[:, :128].pow(2) + g_neg[:, 128:].pow(2)\n",
    "                g_pos_sum = torch.cat([g_pos_sum, g_pos_sum], 1)\n",
    "                g_neg_sum = torch.cat([g_neg_sum, g_neg_sum], 1)\n",
    "                g_pos = torch.div(g_pos, g_pos_sum)\n",
    "                g_neg = torch.div(g_neg, g_neg_sum)\n",
    "                loss = torch.relu(-(g_pos - g_neg).pow(2)).mean()\n",
    "                self.opt.zero_grad()\n",
    "                loss.backward()\n",
    "                self.opt.step()\n",
    "        else:\n",
    "            for _ in tqdm(range(self.num_epochs)):\n",
    "                g_pos = self.forward(x_pos)\n",
    "                g_neg = self.forward(x_neg)\n",
    "                g_pos_sum = g_pos[:, :128].pow(2) + g_pos[:, 128:].pow(2)\n",
    "                g_neg_sum = g_neg[:, :128].pow(2) + g_neg[:, 128:].pow(2)\n",
    "                g_pos_sum = torch.cat([g_pos_sum, g_pos_sum], 1)\n",
    "                g_neg_sum = torch.cat([g_neg_sum, g_neg_sum], 1)\n",
    "                g_pos = torch.div(g_pos, g_pos_sum)\n",
    "                g_neg = torch.div(g_neg, g_neg_sum)\n",
    "                loss = torch.relu(-(g_pos - g_neg).pow(2)).mean()\n",
    "                self.opt.zero_grad()\n",
    "                loss.backward()\n",
    "                self.opt.step()\n",
    "        return self.forward(x_pos).detach(), self.forward(x_neg).detach()\n",
    "\n",
    "    def Channel(self, x): \n",
    "        # add some noise\n",
    "        stddev = np.sqrt(1 / (10 ** (4)))\n",
    "        noise = torch.normal(mean=0, std=stddev, size=x.shape).to(device)\n",
    "        return x + noise\n",
    "    \n",
    "\n",
    "class decoder(nn.Module):\n",
    "    def __init__(self, block_length):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Linear(block_length, 512)\n",
    "        self.conv2 = nn.Linear(512, 256)\n",
    "        self.conv3 = nn.Linear(256, 128)\n",
    "\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = torch.sigmoid(self.conv1(x))\n",
    "        x = torch.sigmoid(self.conv2(x))\n",
    "        x = torch.sigmoid(self.conv3(x))\n",
    "        return x\n",
    "\n",
    "def Channel(x): \n",
    "    # add some noise\n",
    "    stddev = np.sqrt(1 / (10 ** (4)))\n",
    "    noise = torch.normal(mean=0, std=stddev, size=x.shape).to(device)\n",
    "    return x + noise\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100, 128)\n",
      "(100000, 128)\n",
      "(100000, 128)\n",
      "training layer 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1000 [00:00<?, ?it/s]/home/xingyan/anaconda3/envs/py37/lib/python3.7/site-packages/ipykernel_launcher.py:44: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      " 11%|█         | 110/1000 [00:02<00:22, 39.19it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_42638/3882014524.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0mx_neg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabel_false\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m \u001b[0mnet_in\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_pos\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_neg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/tmp/ipykernel_42638/680238661.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, x_pos, x_neg)\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlayer\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"training layer {i}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m             \u001b[0mh_pos\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh_neg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mh_pos\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh_neg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mLayer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLinear\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_42638/680238661.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, x_pos, x_neg)\u001b[0m\n\u001b[1;32m     69\u001b[0m                 \u001b[0mg_pos_sum\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mg_pos_sum\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mg_pos_sum\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m                 \u001b[0mg_neg_sum\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mg_neg_sum\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mg_neg_sum\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 71\u001b[0;31m                 \u001b[0mg_pos\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdiv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mg_pos\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mg_pos_sum\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     72\u001b[0m                 \u001b[0mg_neg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdiv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mg_neg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mg_neg_sum\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m                 \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mg_pos\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mg_neg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "batch_size = 512\n",
    "block_length = 128\n",
    "\n",
    "train_data = np.random.binomial(1, 0.5, [100000, block_length])\n",
    "label_true, label_false = train_data, train_data\n",
    "np.random.shuffle(label_false)\n",
    "\n",
    "test_data = train_data\n",
    "np.random.shuffle(test_data)\n",
    "test_data = test_data[:100, :]\n",
    "test_label = test_data\n",
    "\n",
    "print(test_data.shape)\n",
    "print(train_data.shape)\n",
    "print(label_true.shape)\n",
    "\n",
    "device = torch.device(\"cuda:3\" if torch.cuda.is_available() else \"cpu\")\n",
    "    \n",
    "net_in = Net([block_length, 256, 256, 256], device)\n",
    "# net_out = Net([block_length, 256, 128, 64, 32, 1], device)\n",
    "\n",
    "x_pos = torch.Tensor(label_true).to(device)\n",
    "x_neg = torch.Tensor(label_false).to(device)\n",
    "            \n",
    "net_in.train(x_pos, x_neg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 20/5000 [00:00<00:27, 182.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current loss: 0.25545522570610046\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 521/5000 [00:06<00:52, 85.80it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current loss: 0.017220621928572655\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 1022/5000 [00:12<00:46, 84.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current loss: 0.0056803859770298\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 1522/5000 [00:18<00:40, 85.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current loss: 0.0026116734370589256\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 2022/5000 [00:24<00:35, 84.88it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current loss: 0.0015556573634967208\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 2522/5000 [00:30<00:29, 84.56it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current loss: 0.0010220810072496533\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 3022/5000 [00:37<00:23, 84.65it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current loss: 0.0007070524152368307\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|███████   | 3522/5000 [00:43<00:17, 84.41it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current loss: 0.0005055690417066216\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 4022/5000 [00:49<00:11, 84.48it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current loss: 0.00037101603811606765\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|█████████ | 4522/5000 [00:55<00:05, 84.28it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current loss: 0.00027591927209869027\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5000/5000 [01:01<00:00, 81.24it/s]\n"
     ]
    }
   ],
   "source": [
    "# imm_data = torch.nn.functional.normalize(net_in.forward(x_pos)).cpu().detach()\n",
    "# imm_label = x_pos.cpu().detach()\n",
    "from copy import deepcopy\n",
    "\n",
    "# data, label = train_data, train_data\n",
    "# data = torch.Tensor(data).to(device)\n",
    "# label = torch.Tensor(label).to(device)\n",
    "# print(data.shape)\n",
    "# print(label.shape)\n",
    "# data = Channel(net_in.forward(x_pos)).detach()\n",
    "data = torch.cat([x_pos, x_pos], 1)\n",
    "label = x_pos.detach()\n",
    "\n",
    "# train_data = np.random.binomial(1, 0.5, [100000, block_length])\n",
    "# train_label = train_data\n",
    "\n",
    "\n",
    "deco = decoder(256).to(device)\n",
    "optimizer = Adam(deco.parameters(),lr=1e-3)\n",
    "crit = nn.MSELoss()\n",
    "\n",
    "for epoch in tqdm(range(5000)):\n",
    "    optimizer.zero_grad()\n",
    "    output = deco(data)\n",
    "    loss = crit(output, label)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if epoch % 500 == 0:\n",
    "        print(f\"current loss: {loss.item()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.0000, 0.0003, 0.0000,  ..., 0.0597, 0.0000, 0.0993],\n",
      "        [0.0000, 0.0135, 0.0036,  ..., 0.0610, 0.0000, 0.0809],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0667, 0.0000, 0.1000],\n",
      "        ...,\n",
      "        [0.0000, 0.0083, 0.0000,  ..., 0.0593, 0.0000, 0.0730],\n",
      "        [0.0000, 0.0005, 0.0000,  ..., 0.0436, 0.0000, 0.0745],\n",
      "        [0.0000, 0.0169, 0.0000,  ..., 0.0743, 0.0000, 0.0655]],\n",
      "       device='cuda:3', grad_fn=<ReluBackward0>)\n",
      "tensor([[-0.0045, -0.0053,  0.0050,  ...,  0.0629, -0.0082,  0.1026],\n",
      "        [-0.0053,  0.0139,  0.0091,  ...,  0.0682, -0.0062,  0.0835],\n",
      "        [-0.0222, -0.0023,  0.0035,  ...,  0.0698,  0.0067,  0.0860],\n",
      "        ...,\n",
      "        [-0.0096,  0.0015,  0.0056,  ...,  0.0637,  0.0086,  0.0809],\n",
      "        [-0.0143, -0.0017, -0.0074,  ...,  0.0301, -0.0089,  0.0808],\n",
      "        [-0.0122,  0.0184, -0.0016,  ...,  0.0741,  0.0043,  0.0718]],\n",
      "       device='cuda:3', grad_fn=<AddBackward0>)\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:3')\n",
      "[[0 0 0 ... 1 1 0]\n",
      " [1 0 0 ... 1 0 1]\n",
      " [0 1 1 ... 1 1 0]\n",
      " ...\n",
      " [1 0 1 ... 0 1 0]\n",
      " [1 1 0 ... 1 1 1]\n",
      " [1 1 1 ... 1 1 1]]\n",
      "[[0. 0. 0. ... 1. 1. 0.]\n",
      " [1. 0. 0. ... 1. 0. 1.]\n",
      " [0. 1. 1. ... 1. 1. 0.]\n",
      " ...\n",
      " [1. 0. 1. ... 0. 1. 0.]\n",
      " [1. 1. 0. ... 1. 1. 1.]\n",
      " [1. 1. 1. ... 1. 1. 1.]]\n",
      "[65. 67. 63. 62. 61. 51. 57. 68. 77. 57. 67. 65. 64. 61. 68. 70. 65. 74.\n",
      " 64. 77. 56. 61. 72. 61. 69. 58. 68. 62. 59. 62. 66. 59. 60. 62. 75. 65.\n",
      " 62. 62. 60. 63. 64. 69. 63. 71. 69. 59. 59. 69. 59. 71. 62. 65. 64. 77.\n",
      " 77. 59. 59. 64. 72. 57. 62. 68. 66. 62. 71. 78. 61. 66. 58. 60. 78. 71.\n",
      " 65. 70. 72. 66. 65. 61. 73. 63. 72. 62. 66. 72. 60. 59. 62. 71. 70. 65.\n",
      " 56. 66. 63. 64. 66. 60. 64. 63. 69. 59.]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/xingyan/anaconda3/envs/py37/lib/python3.7/site-packages/ipykernel_launcher.py:44: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n"
     ]
    }
   ],
   "source": [
    "# test_data = np.random.binomial(1, 0.5, [100, block_length])\n",
    "test_label = test_data\n",
    "\n",
    "x_pos = torch.Tensor(test_data).to(device)\n",
    "\n",
    "encoded_msg = net_in.forward(x_pos)\n",
    "\n",
    "print(encoded_msg)\n",
    "\n",
    "channeled_msg = Channel(encoded_msg).to(device)\n",
    "\n",
    "print(channeled_msg)\n",
    "\n",
    "channeled_msg = torch.cat([x_pos, x_pos], 1)\n",
    "output = deco(channeled_msg)\n",
    "output = torch.Tensor(np.where(output.cpu().detach().numpy() >= 0.5, 1, 0)).to(device)\n",
    "\n",
    "print(output)\n",
    "print(test_label)\n",
    "    \n",
    "print(test_label - output.cpu().detach().numpy())\n",
    "print(np.absolute(test_label - output.cpu().detach().numpy()).sum(1))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.15 ('py37': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "9a243070f551174a5465f1a9e3c098ebf386344fd1ca5dc0370448ed621039df"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
