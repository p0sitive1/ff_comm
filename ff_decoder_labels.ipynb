{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/xingyan/anaconda3/envs/py37/lib/python3.7/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from tqdm import tqdm\n",
    "from torch.optim import Adam\n",
    "from torchvision.transforms import Compose, ToTensor, Normalize, Lambda\n",
    "import numpy as np\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Net(torch.nn.Module):\n",
    "    def __init__(self, dims, device=None):\n",
    "        super().__init__()\n",
    "        self.layers = []\n",
    "        for d in range(len(dims) - 1):\n",
    "            self.layers += [Layer(dims[d], dims[d + 1], device=device).cuda()]\n",
    "\n",
    "    def predict(self, x):\n",
    "        goodness_per_label = []\n",
    "        for label in range(16):\n",
    "            h = x\n",
    "            goodness = []\n",
    "            for layer in self.layers:\n",
    "                h = layer(h)\n",
    "                goodness += [h.pow(2).mean(1)]\n",
    "            goodness_per_label += [sum(goodness).unsqueeze(1)]\n",
    "        goodness_per_label = torch.cat(goodness_per_label, 1)\n",
    "        return goodness_per_label.argmax(1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        return x\n",
    "\n",
    "    def train(self, x_pos, x_neg):\n",
    "        h_pos, h_neg = x_pos, x_neg\n",
    "        for i, layer in enumerate(self.layers):\n",
    "            print(f\"training layer {i}\")\n",
    "            h_pos, h_neg = layer.train(h_pos, h_neg)\n",
    "\n",
    "class Layer(nn.Linear):\n",
    "    def __init__(self, in_features, out_features,\n",
    "                 bias=True, device=None, dtype=None):\n",
    "        super().__init__(in_features, out_features, bias, device, dtype)\n",
    "        self.relu = torch.nn.ReLU()\n",
    "        self.opt = Adam(self.parameters(), lr=0.03)\n",
    "        self.threshold = 2.0\n",
    "        self.num_epochs = 100000\n",
    "        self.device = device\n",
    "\n",
    "    def forward(self, x):\n",
    "        if len(x.shape) == 1:\n",
    "            x_direction = torch.nn.functional.normalize(x, dim=0)\n",
    "            x_direction = x_direction.unsqueeze(0)\n",
    "        else:\n",
    "            x_direction = torch.nn.functional.normalize(x)\n",
    "        x_direction = torch.tensor(x_direction, dtype=torch.float32).to(self.device)\n",
    "        return self.relu(\n",
    "            torch.mm(x_direction, self.weight.T) + self.bias.unsqueeze(0))\n",
    "\n",
    "    def train(self, x_pos, x_neg):\n",
    "        for _ in tqdm(range(self.num_epochs)):\n",
    "            g_pos = self.forward(x_pos).pow(2).mean(1)\n",
    "            g_neg = self.forward(x_neg).pow(2).mean(1)\n",
    "            loss = torch.log(1 + torch.exp(torch.cat([\n",
    "                -g_pos + self.threshold,\n",
    "                g_neg - self.threshold]))).mean()\n",
    "            self.opt.zero_grad()\n",
    "            loss.backward()\n",
    "            self.opt.step()\n",
    "        return self.forward(x_pos).detach(), self.forward(x_neg).detach()\n",
    "\n",
    "def Channel(x): \n",
    "    # add some noise\n",
    "    stddev = np.sqrt(1 / (10 ** (6)))\n",
    "    noise = torch.normal(mean=0, std=stddev, size=x.shape).to(device)\n",
    "    return x + noise\n",
    "\n",
    "def dec_to_bin(input, out_length=4):\n",
    "    tmp = str(bin(input))[2:]\n",
    "    if len(tmp) > out_length:\n",
    "        raise(\"input length too short\")\n",
    "    if len(tmp) < out_length:\n",
    "        tmp = (out_length - len(tmp)) * \"0\" + tmp\n",
    "    output = list()\n",
    "    for i in range(out_length):\n",
    "        output.append(int(tmp[i]))\n",
    "    return np.array(output)\n",
    "\n",
    "def generate_data(length):\n",
    "    \"\"\"\n",
    "    generate training data, returns true label, false label, data\n",
    "    \"\"\"\n",
    "    data_true = list()\n",
    "    data_false = list()\n",
    "    data_value = list()\n",
    "    for _ in range(length):\n",
    "        value = random.randrange(0, 16)\n",
    "        label_true = value\n",
    "        label_false = random.randrange(0, 16)\n",
    "        while label_false == value:\n",
    "            label_false = random.randrange(0, 16)\n",
    "\n",
    "        value = dec_to_bin(value)\n",
    "        label_true = dec_to_bin(label_true)\n",
    "        label_false = dec_to_bin(label_false)\n",
    "\n",
    "        data_true.append(label_true)\n",
    "        data_false.append(label_false)\n",
    "        data_value.append(value)\n",
    "    return np.array(data_true), np.array(data_false), np.array(data_value)\n",
    "\n",
    "def generate_spec_data(label, data):\n",
    "    bi_label = dec_to_bin(label)\n",
    "    bi_data = dec_to_bin(data)\n",
    "\n",
    "    return np.array(bi_label), np.array(bi_data)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training layer 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/100000 [00:00<?, ?it/s]/home/xingyan/anaconda3/envs/py37/lib/python3.7/site-packages/ipykernel_launcher.py:47: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "100%|██████████| 100000/100000 [02:55<00:00, 570.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training layer 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100000/100000 [02:49<00:00, 590.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training layer 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100000/100000 [02:49<00:00, 588.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training layer 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100000/100000 [03:04<00:00, 542.64it/s]\n"
     ]
    }
   ],
   "source": [
    "label_true, label_false, data = generate_data(50000)\n",
    "\n",
    "K = 16\n",
    "N = 32\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    \n",
    "net_in = Net([8, 32, 14], device)\n",
    "net_out = Net([14 + 4, 32, 16], device)\n",
    "\n",
    "# training here\n",
    "x_pos = np.concatenate((label_true, data), axis=1)\n",
    "x_neg = np.concatenate((label_false, data), axis=1)\n",
    "\n",
    "label_true = torch.Tensor(label_true).to(device)\n",
    "label_false = torch.Tensor(label_false).to(device)\n",
    "\n",
    "x_pos = torch.Tensor(x_pos).to(device)\n",
    "x_neg = torch.Tensor(x_neg).to(device)\n",
    "    \n",
    "net_in.train(x_pos, x_neg)\n",
    "\n",
    "imm_pos = Channel(net_in.forward(x_pos))\n",
    "imm_neg = Channel(net_in.forward(x_pos))\n",
    "\n",
    "imm_pos = torch.cat((label_true, imm_pos), dim=1)\n",
    "imm_neg = torch.cat((label_false, imm_neg), dim=1)\n",
    "\n",
    "net_out.train(imm_pos, imm_neg)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "testing\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/xingyan/anaconda3/envs/py37/lib/python3.7/site-packages/ipykernel_launcher.py:47: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test error: 0.02\n",
      "top three rate: 1.0\n"
     ]
    }
   ],
   "source": [
    "# testing here\n",
    "tot = 0\n",
    "err = 0\n",
    "t_three = 0\n",
    "print(\"testing\")\n",
    "for _ in range(100):\n",
    "    label_true, label_false, data = generate_data(1)\n",
    "    input = np.concatenate((label_true, data), axis=1)\n",
    "    input = torch.Tensor(np.array(input)).to(device)\n",
    "\n",
    "    imm = Channel(net_in.forward(input))\n",
    "\n",
    "    outputs = list()\n",
    "    for i in range(16):\n",
    "        tmp = torch.Tensor(np.array([dec_to_bin(i)])).to(device)\n",
    "\n",
    "        cur_imm = torch.cat([tmp, imm], dim=1)\n",
    "\n",
    "        output = net_out.forward(cur_imm)\n",
    "\n",
    "        outputs.append(output)\n",
    "        \n",
    "    label_true = label_true.tolist()[0]\n",
    "    for i in range(4):\n",
    "        label_true[i] = str(label_true[i])\n",
    "    ground_truth = \"0b\" + \"\".join(label_true)\n",
    "    ground_truth = int(ground_truth, 2)\n",
    "\n",
    "    guesses = list()\n",
    "    for v in range(len(outputs)):\n",
    "        guesses.append([v, torch.sum(outputs[v]).item()])\n",
    "    guesses.sort(key=lambda x: x[1], reverse=True)\n",
    "    guess = guesses[0][0]\n",
    "\n",
    "    # print(ground_truth)\n",
    "    # print(guesses)\n",
    "\n",
    "    for i in range(3):\n",
    "        if int(ground_truth) == guesses[i][0]:\n",
    "            t_three += 1\n",
    "\n",
    "    if int(ground_truth) == int(guess):\n",
    "        tot += 1\n",
    "    else:\n",
    "        err += 1\n",
    "        tot += 1\n",
    "\n",
    "print(f\"test error: {err/tot}\")\n",
    "print(f\"top three rate: {t_three/tot}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/100 [00:00<?, ?it/s]/home/xingyan/anaconda3/envs/py37/lib/python3.7/site-packages/ipykernel_launcher.py:47: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "100%|██████████| 100/100 [00:19<00:00,  5.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct decoding rate: 0.44\n",
      "errors for the trials: [1, 0, 1, 0, 0, 2, 2, 2, 0, 2, 0, 2, 0, 0, 0, 1, 2, 0, 3, 0, 1, 1, 4, 1, 0, 1, 2, 0, 4, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 3, 1, 2, 0, 1, 1, 1, 1, 1, 1, 1, 0, 4, 2, 1, 5, 0, 0, 2, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 2, 1, 0, 0, 3, 0, 2, 0, 2, 0, 0, 0, 1, 1, 5, 3, 1, 2, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# generating 128 bit data, seperate into 32 4 bit sections, and encode each section seperately\n",
    "corr = 0\n",
    "tot = 0\n",
    "num_errors = []\n",
    "for i in tqdm(range(100)):\n",
    "    # generate data with length n, seperate into m k length sections\n",
    "    n = 128\n",
    "    k = 4\n",
    "    if n % k != 0:\n",
    "        raise(\"Cannot seperate data\")\n",
    "\n",
    "    data = np.random.binomial(1, 0.5, n)\n",
    "    sep_data = np.split(data, n/k)\n",
    "\n",
    "    decoded = list()\n",
    "    for section in sep_data:\n",
    "        input = torch.Tensor(section).to(device)\n",
    "        input = torch.cat([input, input])\n",
    "        imm = Channel(net_in.forward(input))\n",
    "\n",
    "        outputs = list()\n",
    "        for i in range(16):\n",
    "            tmp = torch.Tensor(np.array([dec_to_bin(i)])).to(device)\n",
    "            cur_imm = torch.cat([tmp, imm], dim=1)\n",
    "            output = net_out.forward(cur_imm)\n",
    "            outputs.append(output)\n",
    "        \n",
    "        guesses = list()\n",
    "        for v in range(len(outputs)):\n",
    "            guesses.append([dec_to_bin(v), torch.sum(outputs[v]).item()])\n",
    "        guesses.sort(key=lambda x: x[1], reverse=True)\n",
    "        guess = guesses[0][0]\n",
    "\n",
    "        decoded.append(guess)\n",
    "\n",
    "    decoded = np.concatenate(decoded)\n",
    "\n",
    "    num_errors.append(np.sum(np.absolute(data-decoded)))\n",
    "\n",
    "    if (data==decoded).all():\n",
    "        corr += 1\n",
    "        tot += 1\n",
    "    else:\n",
    "        tot += 1\n",
    "\n",
    "print(f\"Correct decoding rate: {corr/tot}\")\n",
    "print(f\"errors for the trials: {num_errors}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 1 0 1 0 1 1 0 1 1 1 0 0 1 0 1 0 1 1 1 0 1 1 0 1 1 1 1 1 0 0 1 0 1 0 0\n",
      " 1 0 0 1 0 0 1 0 1 1 0 0 1 0 1 1 0 0 1 0 1 1 1 0 1 1 0]\n",
      "[[0, 0, 0, 0], [0, 0, 1, 0], [0, 1, 0, 1], [1, 1, 0, 1], [1, 1, 1, 0], [0, 0, 1, 0], [0, 1, 0, 1], [1, 1, 1, 0], [0, 1, 1, 0], [0, 1, 1, 1], [1, 1, 1, 0], [0, 0, 1, 0], [0, 1, 0, 0], [0, 1, 0, 0], [0, 1, 0, 0], [0, 1, 0, 1], [1, 1, 0, 0], [0, 1, 0, 1], [1, 1, 0, 0], [0, 1, 0, 1], [1, 1, 1, 0], [0, 1, 1, 0], [0, 0, 0, 0]]\n",
      "[[0, 1, 2, 3], [3, 4, 5, 6], [6, 7, 8, 9], [9, 10, 11, 12], [12, 13, 14, 15], [15, 16, 17, 18], [18, 19, 20, 21], [21, 22, 23, 24], [24, 25, 26, 27], [27, 28, 29, 30], [30, 31, 32, 33], [33, 34, 35, 36], [36, 37, 38, 39], [39, 40, 41, 42], [42, 43, 44, 45], [45, 46, 47, 48], [48, 49, 50, 51], [51, 52, 53, 54], [54, 55, 56, 57], [57, 58, 59, 60], [60, 61, 62, 63], [63, 64, 65, 66], [66, 67, 68, 69]]\n",
      "[0 0 1 0 1 0 1 1 0 1 1 1 0 0 1 0 1 0 1 1 1 0 1 1 0 1 1 1 1 1 0 0 1 0 1 0 0\n",
      " 1 0 0 1 0 0 1 0 1 1 0 0 1 0 1 1 0 0 1 0 1 1 1 0 1 1 0]\n"
     ]
    }
   ],
   "source": [
    "# utils for seperating data by convolving \n",
    "data = np.random.binomial(1, 0.5, n)\n",
    "def sep_data_conv(data, padding, stride, n=128, k=4):\n",
    "    sep_data = list()\n",
    "    sep_index = list()\n",
    "    tmp_data = [0] * padding + list(data) + [0] * padding\n",
    "\n",
    "    for i in range(0, len(tmp_data)-padding, stride):\n",
    "        sep_data.append(tmp_data[i: i+k])\n",
    "            \n",
    "    for i in range(0, len(tmp_data)-padding, stride):\n",
    "        sep_index.append([j for j in range(i, i+k)])\n",
    "\n",
    "    return sep_data, sep_index\n",
    "\n",
    "def restore_data_conv(sep_data, sep_index, padding, n=128, k=4):\n",
    "    restored_data = [[] for _ in range(n + padding * 2)]\n",
    "    for i in range(len(sep_data)):\n",
    "        for j in range(len(sep_index[i])):\n",
    "            restored_data[sep_index[i][j]].append(sep_data[i][j])\n",
    "\n",
    "    output = list()\n",
    "    for i in range(padding, n + padding):\n",
    "        tmp = restored_data[i]\n",
    "        ones = tmp.count(1)\n",
    "        zeros = tmp.count(0)\n",
    "        if ones >= zeros:\n",
    "            output.append(1)\n",
    "        else:\n",
    "            output.append(0)\n",
    "\n",
    "    return np.array(output)\n",
    "\n",
    "sep_data, sep_index = sep_data_conv(data, 3, 3)\n",
    "print(data)\n",
    "print(sep_data)\n",
    "print(sep_index)\n",
    "\n",
    "restored_data = restore_data_conv(sep_data, sep_index, 3)\n",
    "print(restored_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/100 [00:00<?, ?it/s]/home/xingyan/anaconda3/envs/py37/lib/python3.7/site-packages/ipykernel_launcher.py:47: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "100%|██████████| 100/100 [00:19<00:00,  5.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct decoding rate: 0.32\n",
      "errors for the trials: [0, 0, 3, 1, 0, 1, 0, 1, 0, 0, 1, 1, 2, 2, 8, 1, 2, 1, 1, 1, 1, 1, 2, 1, 0, 4, 2, 1, 0, 2, 0, 0, 1, 2, 2, 3, 2, 1, 2, 1, 0, 0, 1, 1, 1, 1, 2, 0, 2, 0, 0, 2, 0, 1, 6, 5, 0, 0, 2, 1, 2, 2, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 2, 0, 2, 0, 1, 2, 3, 0, 2, 0, 2, 0, 4, 2, 2, 0, 1, 0, 1, 2, 2, 0, 7, 2, 1]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# generate data with length n, use convolution to create n k length sections\n",
    "n = 128\n",
    "k = 4\n",
    "\n",
    "corr = 0\n",
    "tot = 0\n",
    "num_errors = []\n",
    "for i in tqdm(range(100)): \n",
    "    data = np.random.binomial(1, 0.5, n)\n",
    "    padding = 3\n",
    "    stride = 1\n",
    "    sep_data, sep_index = sep_data_conv(data, padding, stride)\n",
    "\n",
    "    decoded = list()\n",
    "    for section in sep_data:\n",
    "        input = torch.Tensor(section).to(device)\n",
    "        input = torch.cat([input, input])\n",
    "        imm = Channel(net_in.forward(input))\n",
    "\n",
    "        outputs = list()\n",
    "        for i in range(16):\n",
    "            tmp = torch.Tensor(np.array([dec_to_bin(i)])).to(device)\n",
    "            cur_imm = torch.cat([tmp, imm], dim=1)\n",
    "            output = net_out.forward(cur_imm)\n",
    "            outputs.append(output)\n",
    "            \n",
    "        guesses = list()\n",
    "        for v in range(len(outputs)):\n",
    "            guesses.append([dec_to_bin(v), torch.sum(outputs[v]).item()])\n",
    "        guesses.sort(key=lambda x: x[1], reverse=True)\n",
    "        guess = guesses[0][0]\n",
    "        \n",
    "        decoded.append(guess)\n",
    "\n",
    "    output = restore_data_conv(decoded, sep_index, padding)\n",
    "\n",
    "    num_errors.append(np.sum(np.absolute(data-output)))\n",
    "\n",
    "    if (data==output).all():\n",
    "        corr += 1\n",
    "        tot += 1\n",
    "    else:\n",
    "        tot += 1\n",
    "\n",
    "print(f\"Correct decoding rate: {corr/tot}\")\n",
    "print(f\"errors for the trials: {num_errors}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/xingyan/anaconda3/envs/py37/lib/python3.7/site-packages/ipykernel_launcher.py:47: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f20d3c1ba10>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAaAAAAGdCAYAAABU0qcqAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/NK7nSAAAACXBIWXMAAA9hAAAPYQGoP6dpAAAiJklEQVR4nO3dfXRU9b3v8c9kJpmEMBlJkCRTEolKRQER5eEAniMcc6UpoqxetXoRWXiX1jYIiItC2kZbFSLa2giyQLynQu8SH9a6gpZ71UURQZfyGFHpAw+HCBGaIBZnSCCTZGbfP3qScyIJSWD/+GXi+7XW/mP2bD77u4aZfLJndvZ4HMdxBADABZZkewAAwLcTBQQAsIICAgBYQQEBAKyggAAAVlBAAAArKCAAgBUUEADACp/tAb4pHo/r6NGjCgQC8ng8tscBAHSR4zg6efKkQqGQkpLaP87pdgV09OhR5eXl2R4DAHCeqqqq1L9//3bv73YFFAgEJEnX6/vyKdn1/APPXOd6ZrMFN6w3lj029XNj2Z9Gc41lS9K22kuNZVcsHm4s+8j3zF2lKvm4uZde9raYsezTF3uNZf/kodeNZd/R+4SxbEn6t7C519BtgYPGsu+a9YCR3Kameu14t6zl53l7ul0BNb/t5lOyfB73CygpLdX1zGZpvc09nL3TzH1c1yvF3A8VSfIb+EWimS/Z3P9nUpq5AkpKNfdc8SWbKyCvweeKyddPRsDsx91pscSc3eTrR1KHH6NwEgIAwAoKCABgBQUEALCi230GBACwy+OR0nslKy3Vd8bnOI7j6HR9k+pONep8v03OWAEtW7ZMTz/9tKqrqzVs2DAtXbpUo0aNMrU7AIALggG/im68VN+9NEteb9tvksVice07+JXe2nhQ4ZPRc96XkQJ69dVXNXfuXK1YsUKjR49WeXm5Jk6cqL1796pfv34mdgkAOE9er0f/838MUyinj3r1DirJ0/ZZj3EnpoxAukLZAT37v3YoFju3QyEjnwE988wzuu+++zRjxgxdddVVWrFihXr16qXf/e53JnYHAHBBn2CqMgKpSg/0kc+XqiRvcpuLz/ePbTICqeoTPPdTuV0voIaGBu3atUuFhYX/uZOkJBUWFuqjjz46Y/toNKpIJNJqAQBceElJHnk8kkcdXwbNo39sm5R07pdMc72Ajh8/rlgspuzs7Fbrs7OzVV1dfcb2ZWVlCgaDLQuX4QGAbwfrp2GXlJQoHA63LFVVVbZHAgBcAK6fhNC3b195vV7V1NS0Wl9TU6OcnJwztvf7/fL7/W6PAQDo5lw/AkpJSdF1112njRs3tqyLx+PauHGjxowZ4/buAAAJyshp2HPnztX06dM1YsQIjRo1SuXl5aqrq9OMGTNM7A4A4ALHcaSunFHt/Me/OUdGCuiHP/yhvvzySz3yyCOqrq7WNddco7fffvuMExMAAN1HpLZBjU0xxWKNSvKe/Sr2sVijGptiipxsOOf9GbsSwsyZMzVz5kxT8QAAl0WjMW2rOKLxY1N0UR/J204JxWKN+vrEV9pWcUTRhnP/+g+uBQcAaPHH9z+XJI2+tkHJPq/O+JMgR2ps+kdRNW97riggAEALx5E2bPlcW7ZWKSOQ0ubFSCMnG87ryKcZBQQAOEO0IaYvvzptdB/W/xAVAPDt5HHO5xw6AyKRiILBoPo/85iS0tz/vvLv/mS765nNPMMHG8v+cmSGsex+H54wli1JtQODxrJ77w8by1aluatyeLxtX2XYDfHLEvNyVqe/k24sOyXcaCxbkpKP1xrLPnllprHsuKGnYVNjvXauLVU4HFZGRvs/uzgCAgBYQQEBAKyggAAAVlBAAAArKCAAgBUUEADACgoIAGAFBQQAsIICAgBYQQEBAKyggAAAVlBAAAArKCAAgBUUEADACgoIAGAFBQQAsIICAgBYQQEBAKyggAAAVlBAAAArKCAAgBUUEADACp/tAdqz4Ib1Suvt/nivDP9vrmc2cz7+k7HstEtHG8tu7NvLWLYkBT47Ziz7xKhsY9mZdaeNZTtec7/71YwJGsvu++kpY9kpXzcYy/ZF6o1lS5Jz+Kix7PR0v7Hsv12fYSQ3FvV2ajuOgAAAVlBAAAArKCAAgBUUEADACgoIAGAFBQQAsIICAgBY4XoBlZWVaeTIkQoEAurXr5+mTJmivXv3ur0bAECCc72ANm/erOLiYm3dulUbNmxQY2OjbrrpJtXV1bm9KwBAAnP9UgNvv/12q9urVq1Sv379tGvXLv3Lv/yL27sDACQo45fiCYfDkqTMzMw2749Go4pGoy23I5GI6ZEAAN2A0ZMQ4vG45syZo3HjxmnIkCFtblNWVqZgMNiy5OXlmRwJANBNGC2g4uJi7dmzR6+88kq725SUlCgcDrcsVVVVJkcCAHQTxt6CmzlzptavX68tW7aof//+7W7n9/vl95u72isAoHtyvYAcx9GDDz6otWvX6r333lNBQYHbuwAA9ACuF1BxcbHWrFmjN954Q4FAQNXV1ZKkYDCotLQ0t3cHAEhQrn8GtHz5coXDYY0fP165ubkty6uvvur2rgAACczIW3AAAHSEa8EBAKyggAAAVlBAAAArKCAAgBXGrwV3rsamfq7eae7349KRGa5nNku7dLSx7PT/s81YdvT7I41lS1Ls8r7Gsr++3NzvUN5ojrFsT9xYtBqC5rKPX9PLWPbFFeaumN/QN91YtiQ15Q82lt2QYe45Hvluk5Hc+OnO5XIEBACwggICAFhBAQEArKCAAABWUEAAACsoIACAFRQQAMAKCggAYAUFBACwggICAFhBAQEArKCAAABWUEAAACsoIACAFRQQAMAKCggAYAUFBACwggICAFhBAQEArKCAAABWUEAAACsoIACAFT7bA7Tn02iueqV4Xc/t9+EJ1zObNfbtZSw7+v2RxrL9/2+HsWxJavrX64xlpx9xjGVnfHbcWHZjdoax7N5HUo1lR4MeY9nymMuu75tsLFuSghU1xrIbRmYbyzZ2CNLJXI6AAABWUEAAACsoIACAFRQQAMAKCggAYAUFBACwggICAFhhvICefPJJeTwezZkzx/SuAAAJxGgB7dixQ88//7yuvvpqk7sBACQgYwVUW1urqVOn6oUXXlCfPn1M7QYAkKCMFVBxcbEmTZqkwsJCU7sAACQwI9eCe+WVV1RRUaEdOzq+xlg0GlU0Gm25HYlETIwEAOhmXD8Cqqqq0uzZs/XSSy8pNbXjiyKWlZUpGAy2LHl5eW6PBADohlwvoF27dunYsWO69tpr5fP55PP5tHnzZi1ZskQ+n0+xWKzV9iUlJQqHwy1LVVWV2yMBALoh19+Cu/HGG/XZZ5+1WjdjxgwNGjRI8+fPl9fb+isW/H6//H6/22MAALo51wsoEAhoyJAhrdalp6crKyvrjPUAgG8vroQAALDignwj6nvvvXchdgMASCAcAQEArKCAAABWUEAAACsoIACAFRQQAMCKC3IW3LnYVnup/Ep2Pbd2YND1zGaBz44Zy45d3tdYdtO/XmcsW5J87+4ylp3yw38yli2Px1h0UrTJWHbc2/E256rPvgZj2Y0B91/vzZr85v4vJclTb+5x6X002vFG5yiwL81IbizauWrhCAgAYAUFBACwggICAFhBAQEArKCAAABWUEAAACsoIACAFRQQAMAKCggAYAUFBACwggICAFhBAQEArKCAAABWUEAAACsoIACAFRQQAMAKCggAYAUFBACwggICAFhBAQEArKCAAABWUEAAACt8tgdoT8Xi4fIlp7qe2/vfw65nNjsxKttY9teXm/tdIf2IYyxbklJ++E/GsgOvbjWWrSsuNxbtO/KVseysj/cay/b2zTSWfeKGAcay6/t6jGVL0t//Oc9Y9qkcc6/9/r/7k5HcJqdBf+3EdhwBAQCsoIAAAFZQQAAAKyggAIAVFBAAwAoKCABgBQUEALDCSAEdOXJEd999t7KyspSWlqahQ4dq586dJnYFAEhQrv8h6okTJzRu3DhNmDBBb731li6++GLt379fffr0cXtXAIAE5noBLV68WHl5eXrxxRdb1hUUFLi9GwBAgnP9Lbg333xTI0aM0O23365+/fpp+PDheuGFF9rdPhqNKhKJtFoAAD2f6wV08OBBLV++XAMHDtQ777yjH//4x5o1a5ZWr17d5vZlZWUKBoMtS16euWsqAQC6D9cLKB6P69prr9WiRYs0fPhw3X///brvvvu0YsWKNrcvKSlROBxuWaqqqtweCQDQDbleQLm5ubrqqqtarbvyyit1+PDhNrf3+/3KyMhotQAAej7XC2jcuHHau7f15eD37dunSy65xO1dAQASmOsF9NBDD2nr1q1atGiRDhw4oDVr1mjlypUqLi52e1cAgATmegGNHDlSa9eu1csvv6whQ4bo8ccfV3l5uaZOner2rgAACczIN6LefPPNuvnmm01EAwB6CK4FBwCwggICAFhBAQEArKCAAABWGDkJwQ1HvucoKc1xPXfQHHNXWsisO20s2xvNMZad8dlxY9mSJI/HXPYVlxuLju09YCw7qVcvY9meqy4zlu00NBnL7lXdYCy79+GYsWxJSj5YbSy798CQsezaG64wktvUWC/934634wgIAGAFBQQAsIICAgBYQQEBAKyggAAAVlBAAAArKCAAgBUUEADACgoIAGAFBQQAsIICAgBYQQEBAKyggAAAVlBAAAArKCAAgBUUEADACgoIAGAFBQQAsIICAgBYQQEBAKyggAAAVlBAAAArfLYHaE/ycZ+SUt0fz+P1up7ZzPGa63NP3Fi0GrMzzIVLSoo2Gcv2HfnKWHZSr17GsuOnThnL9p5uMJbtOVlnLFs5AWPRSY0GX0CS1CvNWHQ8xdzPlbS/nTaS29RU36ntOAICAFhBAQEArKCAAABWUEAAACsoIACAFRQQAMAKCggAYIXrBRSLxVRaWqqCggKlpaXpsssu0+OPPy7HcdzeFQAggbn+l56LFy/W8uXLtXr1ag0ePFg7d+7UjBkzFAwGNWvWLLd3BwBIUK4X0Icffqhbb71VkyZNkiQNGDBAL7/8srZv3+72rgAACcz1t+DGjh2rjRs3at++fZKkTz75RB988IGKiora3D4ajSoSibRaAAA9n+tHQAsWLFAkEtGgQYPk9XoVi8W0cOFCTZ06tc3ty8rK9Ktf/crtMQAA3ZzrR0CvvfaaXnrpJa1Zs0YVFRVavXq1fv3rX2v16tVtbl9SUqJwONyyVFVVuT0SAKAbcv0IaN68eVqwYIHuvPNOSdLQoUN16NAhlZWVafr06Wds7/f75ff73R4DANDNuX4EdOrUKSUltY71er2Kxw1fDh0AkFBcPwKaPHmyFi5cqPz8fA0ePFgff/yxnnnmGd17771u7woAkMBcL6ClS5eqtLRUP/nJT3Ts2DGFQiH96Ec/0iOPPOL2rgAACcz1AgoEAiovL1d5ebnb0QCAHoRrwQEArKCAAABWUEAAACsoIACAFa6fhOCW7G0x+ZJjrufGL8tzPbNZzZigsewGc9HqfSTVXLikuNdcdtbHe41le666zFi293SDsezYvn83lu0deKmx7PrMZGPZfx9r9jneq6a3sez6vh5j2Zf877+ZCY5HO7UZR0AAACsoIACAFRQQAMAKCggAYAUFBACwggICAFhBAQEArKCAAABWUEAAACsoIACAFRQQAMAKCggAYAUFBACwggICAFhBAQEArKCAAABWUEAAACsoIACAFRQQAMAKCggAYAUFBACwggICAFjhsz1Ae05f7JU3xet6btoR1yNb9P30lLHs49f0MpYdDXqMZUtSn30NxrK9fTONZTsNTcayPSfrjGV7B15qLDu2/6CxbO8VfY1lByvjxrIlyX8iZiy715cGX58xQ3PHO/d4cwQEALCCAgIAWEEBAQCsoIAAAFZQQAAAKyggAIAVFBAAwIouF9CWLVs0efJkhUIheTwerVu3rtX9juPokUceUW5urtLS0lRYWKj9+/e7NS8AoIfocgHV1dVp2LBhWrZsWZv3P/XUU1qyZIlWrFihbdu2KT09XRMnTlR9ff15DwsA6Dm6fCWEoqIiFRUVtXmf4zgqLy/XL37xC916662SpN///vfKzs7WunXrdOedd57ftACAHsPVz4AqKytVXV2twsLClnXBYFCjR4/WRx991Oa/iUajikQirRYAQM/nagFVV1dLkrKzs1utz87Obrnvm8rKyhQMBluWvLw8N0cCAHRT1s+CKykpUTgcblmqqqpsjwQAuABcLaCcnBxJUk1NTav1NTU1Lfd9k9/vV0ZGRqsFANDzuVpABQUFysnJ0caNG1vWRSIRbdu2TWPGjHFzVwCABNfls+Bqa2t14MCBltuVlZXavXu3MjMzlZ+frzlz5uiJJ57QwIEDVVBQoNLSUoVCIU2ZMsXNuQEACa7LBbRz505NmDCh5fbcuXMlSdOnT9eqVav005/+VHV1dbr//vv19ddf6/rrr9fbb7+t1NRU96YGACS8LhfQ+PHj5ThOu/d7PB499thjeuyxx85rMABAz2b9LDgAwLcTBQQAsIICAgBYQQEBAKzwOGc7o8CCSCSiYDCo5yuuU1rvLp8j0aGVD/931zObpXzdYCzbEzf43+TxmMuW1BhINpYdvchrLLtXtbn/Txl8yOszzT3e3qi552Hq+u3Gshu+N9JYtiT5TjcZzTfl+OA0I7mxhnr96YWfKRwOn/XiAhwBAQCsoIAAAFZQQAAAKyggAIAVFBAAwAoKCABgBQUEALCCAgIAWEEBAQCsoIAAAFZQQAAAKyggAIAVFBAAwAoKCABgBQUEALCCAgIAWEEBAQCsoIAAAFZQQAAAKyggAIAVFBAAwAoKCABghc/2AO25o/cJZQTc78dV4UbXM5v5IvXGshv6phvLru+bbCxbkpr8HmPZ9X3NZfc+HDOWndQYN5b997GpxrKDlebmTvreSGPZKW/vMJYtSfEbhhvNN+VUrmMkN17fuVyOgAAAVlBAAAArKCAAgBUUEADACgoIAGAFBQQAsIICAgBY0eUC2rJliyZPnqxQKCSPx6N169a13NfY2Kj58+dr6NChSk9PVygU0j333KOjR4+6OTMAoAfocgHV1dVp2LBhWrZs2Rn3nTp1ShUVFSotLVVFRYVef/117d27V7fccosrwwIAeo4uXwmhqKhIRUVFbd4XDAa1YcOGVuuee+45jRo1SocPH1Z+fv65TQkA6HGMX4onHA7L4/HooosuavP+aDSqaDTacjsSiZgeCQDQDRg9CaG+vl7z58/XXXfdpYyMjDa3KSsrUzAYbFny8vJMjgQA6CaMFVBjY6PuuOMOOY6j5cuXt7tdSUmJwuFwy1JVVWVqJABAN2LkLbjm8jl06JDefffddo9+JMnv98vv95sYAwDQjbleQM3ls3//fm3atElZWVlu7wIA0AN0uYBqa2t14MCBltuVlZXavXu3MjMzlZubq9tuu00VFRVav369YrGYqqurJUmZmZlKSUlxb3IAQELrcgHt3LlTEyZMaLk9d+5cSdL06dP1y1/+Um+++aYk6Zprrmn17zZt2qTx48ef+6QAgB6lywU0fvx4OU7733Z3tvsAAGjGteAAAFZQQAAAKyggAIAVFBAAwAoKCABghfGLkZ6rfwvnKi3m/njJx2tdz2zmHDb3vUdN+YONZQcraoxlS5KnvsFY9t//2dy1A5MPVhvLVq80c9E1vY1l+0/EjGX7TjcZy47fMNxYtiQlbf7YWLZn5FBj2d4Gj5ngxs7lcgQEALCCAgIAWEEBAQCsoIAAAFZQQAAAKyggAIAVFBAAwAoKCABgBQUEALCCAgIAWEEBAQCsoIAAAFZQQAAAKyggAIAVFBAAwAoKCABgBQUEALCCAgIAWEEBAQCsoIAAAFZQQAAAKyggAIAVPtsDtOe2wEFlBNzvx5euvNn1zGbp6X5j2Q0Z5n5XaBiZbSxbknofjRrLPpVj7nHpPTBkLDueYm7u+r4eY9m9vjSXrVPmok3zjBxqLNvZ8Zm57KKxZnI7+TThCAgAYAUFBACwggICAFhBAQEArKCAAABWUEAAACu6XEBbtmzR5MmTFQqF5PF4tG7duna3feCBB+TxeFReXn4eIwIAeqIuF1BdXZ2GDRumZcuWnXW7tWvXauvWrQqFzP0tBQAgcXX5D1GLiopUVFR01m2OHDmiBx98UO+8844mTZp0zsMBAHou1z8DisfjmjZtmubNm6fBgwe7HQ8A6CFcvxTP4sWL5fP5NGvWrE5tH41GFY3+56VaIpGI2yMBALohV4+Adu3apWeffVarVq2Sx9O5iwGVlZUpGAy2LHl5eW6OBADoplwtoPfff1/Hjh1Tfn6+fD6ffD6fDh06pIcfflgDBgxo89+UlJQoHA63LFVVVW6OBADoplx9C27atGkqLCxstW7ixImaNm2aZsyY0ea/8fv98vvNXUUaANA9dbmAamtrdeDAgZbblZWV2r17tzIzM5Wfn6+srKxW2ycnJysnJ0dXXHHF+U8LAOgxulxAO3fu1IQJE1puz507V5I0ffp0rVq1yrXBAAA9W5cLaPz48XIcp9Pbf/75513dBQDgW4BrwQEArKCAAABWUEAAACsoIACAFRQQAMAKj9OVU9ougEgkomAwqDE3/Uq+5FTX8xvTzXVu7Xe8xrIj320ylm3615DAPtcvOdii/+/+ZCy79gZzf7uW9rfTxrJ9R74ylq1YzFj00R9cZiz7VK7ZH3Pehs5deuxcOOailf/Yh0Zym5xGvac3FA6HlZGR0e52HAEBAKyggAAAVlBAAAArKCAAgBUUEADACgoIAGAFBQQAsIICAgBYQQEBAKyggAAAVlBAAAArKCAAgBUUEADACgoIAGAFBQQAsIICAgBYQQEBAKyggAAAVlBAAAArKCAAgBUUEADACp/tAb7JcRxJUlNTvZH8pkZznRuLeo1lx083Gcs2/WtILGruadbkNJjLbjTzHJTMPb8lSfGowey4sehYg7nHJF7vGMuWJDV6jEU75qLV5DSaydU/cpt/nrfH43S0xQX2xRdfKC8vz/YYAIDzVFVVpf79+7d7f7croHg8rqNHjyoQCMjj6bj6I5GI8vLyVFVVpYyMjAswoTuY+8JK1LmlxJ2duS+s7jS34zg6efKkQqGQkpLaf4ul270Fl5SUdNbGbE9GRob1B/1cMPeFlahzS4k7O3NfWN1l7mAw2OE2nIQAALCCAgIAWJHwBeT3+/Xoo4/K7/fbHqVLmPvCStS5pcSdnbkvrEScu9udhAAA+HZI+CMgAEBiooAAAFZQQAAAKyggAIAVCV1Ay5Yt04ABA5SamqrRo0dr+/bttkfqUFlZmUaOHKlAIKB+/fppypQp2rt3r+2xuuzJJ5+Ux+PRnDlzbI/SoSNHjujuu+9WVlaW0tLSNHToUO3cudP2WGcVi8VUWlqqgoICpaWl6bLLLtPjjz/e4bW1bNiyZYsmT56sUCgkj8ejdevWtbrfcRw98sgjys3NVVpamgoLC7V//347w/4XZ5u7sbFR8+fP19ChQ5Wenq5QKKR77rlHR48etTfwf+jo8f6vHnjgAXk8HpWXl1+w+boiYQvo1Vdf1dy5c/Xoo4+qoqJCw4YN08SJE3Xs2DHbo53V5s2bVVxcrK1bt2rDhg1qbGzUTTfdpLq6OtujddqOHTv0/PPP6+qrr7Y9SodOnDihcePGKTk5WW+99Zb+/Oc/6ze/+Y369Olje7SzWrx4sZYvX67nnntOf/nLX7R48WI99dRTWrp0qe3RzlBXV6dhw4Zp2bJlbd7/1FNPacmSJVqxYoW2bdum9PR0TZw4UfX1Bi/I2glnm/vUqVOqqKhQaWmpKioq9Prrr2vv3r265ZZbLEzaWkePd7O1a9dq69atCoVCF2iyc+AkqFGjRjnFxcUtt2OxmBMKhZyysjKLU3XdsWPHHEnO5s2bbY/SKSdPnnQGDhzobNiwwbnhhhuc2bNn2x7prObPn+9cf/31tsfoskmTJjn33ntvq3U/+MEPnKlTp1qaqHMkOWvXrm25HY/HnZycHOfpp59uWff11187fr/fefnlly1M2LZvzt2W7du3O5KcQ4cOXZihOqG9ub/44gvnO9/5jrNnzx7nkksucX77299e8Nk6IyGPgBoaGrRr1y4VFha2rEtKSlJhYaE++ugji5N1XTgcliRlZmZanqRziouLNWnSpFaPfXf25ptvasSIEbr99tvVr18/DR8+XC+88ILtsTo0duxYbdy4Ufv27ZMkffLJJ/rggw9UVFRkebKuqaysVHV1davnSzAY1OjRoxPyterxeHTRRRfZHuWs4vG4pk2bpnnz5mnw4MG2xzmrbncx0s44fvy4YrGYsrOzW63Pzs7WX//6V0tTdV08HtecOXM0btw4DRkyxPY4HXrllVdUUVGhHTt22B6l0w4ePKjly5dr7ty5+tnPfqYdO3Zo1qxZSklJ0fTp022P164FCxYoEolo0KBB8nq9isViWrhwoaZOnWp7tC6prq6WpDZfq833JYL6+nrNnz9fd911V7e40OfZLF68WD6fT7NmzbI9SocSsoB6iuLiYu3Zs0cffPCB7VE6VFVVpdmzZ2vDhg1KTU21PU6nxeNxjRgxQosWLZIkDR8+XHv27NGKFSu6dQG99tpreumll7RmzRoNHjxYu3fv1pw5cxQKhbr13D1RY2Oj7rjjDjmOo+XLl9se56x27dqlZ599VhUVFZ36OhvbEvItuL59+8rr9aqmpqbV+pqaGuXk5Fiaqmtmzpyp9evXa9OmTef09RMX2q5du3Ts2DFde+218vl88vl82rx5s5YsWSKfz6dYLGZ7xDbl5ubqqquuarXuyiuv1OHDhy1N1Dnz5s3TggULdOedd2ro0KGaNm2aHnroIZWVldkerUuaX4+J+lptLp9Dhw5pw4YN3f7o5/3339exY8eUn5/f8jo9dOiQHn74YQ0YMMD2eGdIyAJKSUnRddddp40bN7asi8fj2rhxo8aMGWNxso45jqOZM2dq7dq1evfdd1VQUGB7pE658cYb9dlnn2n37t0ty4gRIzR16lTt3r1bXq+5ryM/H+PGjTvjNPd9+/bpkksusTRR55w6deqML/Lyer2KG/xKbBMKCgqUk5PT6rUaiUS0bdu2bv9abS6f/fv3649//KOysrJsj9ShadOm6dNPP231Og2FQpo3b57eeecd2+OdIWHfgps7d66mT5+uESNGaNSoUSovL1ddXZ1mzJhhe7SzKi4u1po1a/TGG28oEAi0vA8eDAaVlpZmebr2BQKBMz6nSk9PV1ZWVrf+/Oqhhx7S2LFjtWjRIt1xxx3avn27Vq5cqZUrV9oe7awmT56shQsXKj8/X4MHD9bHH3+sZ555Rvfee6/t0c5QW1urAwcOtNyurKzU7t27lZmZqfz8fM2ZM0dPPPGEBg4cqIKCApWWlioUCmnKlCn2htbZ587NzdVtt92miooKrV+/XrFYrOW1mpmZqZSUFFtjd/h4f7Mok5OTlZOToyuuuOJCj9ox26fhnY+lS5c6+fn5TkpKijNq1Chn69attkfqkKQ2lxdffNH2aF2WCKdhO47j/OEPf3CGDBni+P1+Z9CgQc7KlSttj9ShSCTizJ4928nPz3dSU1OdSy+91Pn5z3/uRKNR26OdYdOmTW0+p6dPn+44zj9OxS4tLXWys7Mdv9/v3Hjjjc7evXvtDu2cfe7Kysp2X6ubNm3qtnO3pTufhs3XMQAArEjIz4AAAImPAgIAWEEBAQCsoIAAAFZQQAAAKyggAIAVFBAAwAoKCABgBQUEALCCAgIAWEEBAQCsoIAAAFb8fy2WtR+8JJsqAAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "res = np.zeros((16,16))\n",
    "codeword = np.zeros((16,14))\n",
    "for i in range(16):\n",
    "    # test_data = np.random.binomial(1, 0.5, [1, block_length])\n",
    "    test_data = torch.Tensor(dec_to_bin(i)).unsqueeze(0)\n",
    "    # print(test_data)\n",
    "    label_true = test_data\n",
    "\n",
    "    test_data = torch.Tensor(test_data).to(device)\n",
    "    label_true = torch.Tensor(label_true).to(device)\n",
    "\n",
    "    input = torch.cat([label_true, test_data], dim=1)\n",
    "\n",
    "    imm_no_channel = net_in.forward(input)\n",
    "    # imm_no_channel /= torch.sum(imm_no_channel**2)\n",
    "    codeword[i,:] = imm_no_channel.cpu().detach().numpy()\n",
    "for i in range(16):\n",
    "    for j in range(16):\n",
    "        res[i,j]= np.sum((codeword[i]-codeword[j])**2)\n",
    "\n",
    "plt.legend()\n",
    "plt.imshow(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([26.33681014,  3.7794583 ,  3.94444691, 20.27606389, 11.96544109,\n",
       "        9.06688867,  1.94717957,  3.55755718,  3.55755718,  1.94717957,\n",
       "        7.84073798, 20.27606389, 15.7241279 ,  3.94444691, 17.78518779,\n",
       "        5.20273887])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for i in range(16):\n",
    "    res[i,i]+=100\n",
    "np.min(res,axis=0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.15 ('py37': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "9a243070f551174a5465f1a9e3c098ebf386344fd1ca5dc0370448ed621039df"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
